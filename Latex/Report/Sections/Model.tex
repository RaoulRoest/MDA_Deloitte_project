In the previous chapter, the data has been analysed. Now a model needs 
to be chosen to estimate the probabilities of prepayment. In similar 
instances, the multinomial logistic model is often used. Also, this 
model is suggested by the company. For this reason this model is 
chosen 

\subsection{Model Description}
    % Modelling the prepayment probabilities. 
    The model will predict prepayment probabilities. 
    Generally, results of the model will be obtained in the following 
    way. 
    First, there is a variable selection. Based on this variable
    selection, certain features will be used in predicting the prepayment  
    probabilities. The selected features will be on a client level and 
    can be used to estimate the specific probabilities of prepayment 
    on a client level. Both the full and the partial prepayment will 
    be estimated, i.e. the probability that a client with certain 
    characteristics will do a partial or full prepayment. 
    As the data also includes time dependence, a set up which also 
    takes time into consideration will be discussed. To this extend, 
    suppose a portfolio $P$ with client data, a time step $t$ on 
    which the estimation will be performed and a future time horizon $T$. 
    Using the data up to the moment $t$, for the whole portfolio, 
    the model will perform an estimation of the probabilities per 
    client over the given time horizon $T$. 
    \\\\ 
    % Getting the input data
    The total data set can be splitted in two data sets, one for the 
    client data given at the moment of take out of the mortgage 
    (which will be referred at as
    the originate data set) and one which contains information of 
    the mortgage over time (which will be referred to as the monthly 
    data set). For a client $i$ in $P$ the data provides certain 
    information on the moment of take out of the mortgage; 
    $x_1^i, \ldots, x_n^i$ and time dependent information $x_1^{t, i}, 
    \ldots, x_m^{t, i}$. As for all the clients, the same procedure 
    is used, the $i$ is omitted for convenience, hence 
    $x_k^i \equiv x_k$ and $x_l^{t, i} \equiv x_l^t$ for each 
    $k \in \{1, \ldots, n\}$ and $l \in \{1, 
    \ldots m \}$.  The fixed time information can be inferred
    from the originate data and the time dependent data can be 
    inferred from the monthly data. As the model evaluate the data on 
    a specific time step, it can evaluate the features 
    $x_1, \ldots, x_n$ directly. On the other hand, the data
    which depends on time cannot be used directly and needs to be 
    transformed. This transformation will be such that each feature 
    on a certain time step includes all the information up to that 
    moment. To get this information, a certain condition $c_k$ is set. 
    For all time steps $t_i < t$, this condition will be verified 
    over the data. To bring it to one number on time $t$, the 
    evaluations of the condition over all time steps will be aggregated 
    using some aggregation operator. To make this more clear, consider 
    the following example. Suppose a client $i \in P$ and the feature 
    $x^t$, which describes if the prepayment type (i.e. no, partial or 
    full) at time $t$. Now, consider the condition $c_k$:
    \begin{equation}
        c_k(x^t) = \left\{
        \begin{array}{l l l l l}
            1 & \text{if there is a partial prepayment} \\ 
            0 & \text{else}
        \end{array}
        \right.
    \end{equation}
    Evaluating this condition, over all time steps up to the moment 
    $t$ and summing over those evaluations, yield a new feature $x$. 
    This feature represents the count of partial prepayments up to time 
    $t$ for that client. Now $x$ can be used as a feature to predict 
    the prepayment probabilities for this client. 
    \\\\
    % Addressing the output
    As described above, the output of the model will be probabilities 
    which are related to the prepayment risk of a specific client. More 
    specifically, suppose a random variable $Y$ which can take values in 
    $\{0, 1, 2\}$ where $Y=0$ is no prepayment, 
    $Y=1$ is a partial prepayment and $Y=2$ represents a full prepayment. 
    The model will return the probabilities 
    $P(Y = 0 | X) = p_0, P(Y = 1 | X) = p_1$ and 
    $P(Y = 2 | X) = p_2$
    for given features $X = [x_1, \ldots, x_{n+e}]$ (where the number of 
    features 
    $n+e$ is based on the derived features from the monthly data and 
    the variable selection). The predicted probability 
    $p_j, \ j \in \{0, 1, 2\}$ is the probability 
    that that prepayment state will be hit by the client within the 
    interval $[t, T]$. 

\subsection{Multinomial logistic model}
    For the purposes of this assignment, we used the multinomial logit model; 
    a generalisation of the binomial logistic regression for a target 
    variable of more than two classes. In our case, the classes we have are three: No Prepayment, Partial Prepayment and Full Prepayment. We considered a nominal model instead of an ordinal one because, despite the fact that a Full Prepayment is a prepayment of higher magnitute than a Partial Prepayment, they are events that differ in nature. 
    
    We assume that our categories are encoded as 0 for No prepayment, 1 for Partial Prepayment and 2 for Full Prepayment. 0 is going to be our reference class. Therefore, our model will yield to logit functions, one for each of the other two classes and the probability of class 0 will be calculated by subtracting the sum of the two probabilities from 1.
    
    The two logit functions, $g_1, g_2$ will then be:
    \begin{equation}
        g_1(x) = ln\Big[\frac{Pr(Y=1|x)}{Pr(Y=0|x)}\Big] = \beta_{10} + \beta_{11}x_1 + \beta_{12}x_2 +...+ \beta_{1p}x_p = x^T\beta_1
    \end{equation}
    
    \begin{equation}
        g_2(x) = ln\Big[\frac{Pr(Y=2|x)}{Pr(Y=0|x)}\Big] = \beta_{20} + \beta_{21}x_1 + \beta_{22}x_2 +...+ \beta_{2p}x_p = x^T\beta_2
    \end{equation}
    
    If we then consider $g_0 = 0$ the conditional probabilities for each class j, $j=0,1,2$ will be given by:
    
    \begin{equation}
        p_j(x) = Pr(Y=j|x) = \frac{e^{g_j(x)}}{\sum_{j=0}^{2} e^{g_j(x)}}
    \end{equation}
    
    The coefficients vector are then computed after deriving the maximum likelihood. The likelihood is constructed as follows. $Y=0$ when $Y_0=1$, $Y_1=Y_2=0$, $Y=1$ when $Y_1=1$, $Y_0=Y_2=0$ and $Y=2$ when $Y_2=1$ and $Y_0=Y_1=0$. Then, the conditional likelihood is derived by:
    \begin{equation}
        l(\beta) = \prod_{i=1}^{n}[p_0(x_i)^{y_{0i}}p_1(x_i)^{y_{1i}}p_2(x_i)^{y_{2i}}] 
    \end{equation}
    
    Then the log-likelihood is:
    \begin{equation}
        L(\beta) = \sum_{j=1}^{n} y_{1i}g_1(x_i) + y_{2i}g_2(x_i) - ln(1 + e^{g_1(x)} + e^{g_2(x)})
    \end{equation}
    
    By taking partial derivatives one can then compute the maximum likelihood and therefore the $\beta$ coefficients. 
    
    
    % \textcolor{red}{(Theory part)}

    Initially, we aggregated all our originate data, that is the data 
    sets that contain information for specific loans as mentioned in 
    Section \ref{section_data_analysis}.

    We then split the data set into two separate ones. A train set 
    which contained 70\% of the observations of the data set and which 
    we used to train our model and a test set, containing the rest of 
    the observations and which we used to assess the performance of the 
    model.
    Because of the nature of our variables, even after the random split of our dataset, we 

    % \textcolor{red}{\subsection{Possible second model if time allows}}


\subsection{Variable selection}
    For reasons of computational time, to avoid possible overfitting 
    and to have an alternative model to compare with our initial one, 
    we thought of selecting a subset of feature. We used the LASSO 
    multinomial regression. Lasso stands for least absolute shrinkage 
    and selection operator. Multinomial LASSO, just as LASSO regression 
    tends to shrink not important variables and keep only those that 
    are significant in the final model. 
    %For a system of equations 
    %\begin{equation}
     %   Y=X\beta +\epsilon  
    %\end{equation}
    %the lasso estimator is defined by
    %\begin{equation}
     %   \min_\beta(||Y-X\beta||^2+\lambda||\beta||_1).  
    %\end{equation} 
    %$\lambda$ denotes the penalty given by lasso regression and 
    %determines the bias of the estimator. 
    In the multinomial case we consider an extension of the method. 
    In this case, there are more than one levels need to consider. 
    While working with $K$ levels, in the multinomial case, one uses 
    the following probability function:
    \begin{equation}
        Pr(G=k|X=x)=\frac{e^{\beta_{0k}+\beta_k^T x}} {\sum_{l=1}^K e^{\beta_{0l}+\beta_l^T x}}  
    \end{equation}
    By setting $p_l(x_i)=Pr(G=l|x_i)$, $g_i\in \{1,2, \hdots, K\}$ and to
    maximize the log-likelihood after taking partial derivatives, one needs to maximize \parencite{Friedman_2010}: 
    \begin{equation}
        \max_{\{\beta_{0k},\beta_k\}_1^K}
        \left[
            \frac{1}{N}\sum_{i=1}^N 
            \log(p_{g_i}(x_i))-\lambda \sum_{l=1}^K P_\alpha (\beta_l) 
        \right].  
    \end{equation}
    This function can be rewritten as
    \begin{equation}
        l(\{\beta_{0k},\beta_k\}_1^K)=
        -\left[ 
            \frac{1}{N} \sum_{i=1}^N
            \left( 
                \sum_{k=1}^Ky_{il}(\beta_{0k}+x_i^T\beta_k)-
                \log\left(
                    \sum_{l=1}^K e^{\beta_{0l}+x_i^T\beta_l}
                \right)
            \right)
        \right],  
    \end{equation} 
    in which $Y$ is the $N\times K$ indicator response matrix. 
    Matrix $Y$ consists of $y_{ik}=I(g_i=l)$ \parencite{Friedman_2010}. 
    \\\\
    In this case, the penalized negative log-likelihood 
    function for the LASSO case in the elastic net context is given by 
    \parencite{hastie2016introduction}:
    \begin{equation}
        l(\{\beta_{0k},\beta_k\}_1^K)=
        -\left[ 
            \frac{1}{N} \sum_{i=1}^N\left( 
                \sum_{k=1}^Ky_{il}(\beta_{0k}+x_i^T\beta_k)-
                log\left(
                    \sum_{l=1}^K e^{\beta_{0l}+x_i^T\beta_l}
                \right)
            \right)
        \right] + \lambda \sum_{j=1}^p ||\beta_j||_q.  
    \end{equation}

    % \textcolor{red}{Variable selection obtained with lasso}

\subsection{Performance criteria}
    % \textcolor{red}{ROC-AUC}
    To assess the performance of our model we used an extension of 
    the ROC curve for multiple classes. In the typical binomial ROC 
    plot, we can calculate the AUC, that is the Area Under the (ROC) 
    Curve, as follows. Consider $p(x)$ the probability that x belongs 
    to class 0. If $f(p)=f(p(x)|0)$ the probability function of 0 
    class points belonging to class 0 and $g(p(x)|1)$ the probability 
    function of class 1 points belonging to class 0, then we can plot 
    their corresponding cumulative distributions F and G against each 
    other.
    
    The resulting plot with $G(p)$ on the vertical axis and $F(p)$ on 
    the horizontal is the ROC curve. If the ROC curve lies above the 
    positive slope diagonal, then we have a better than random model. 
    The area under the curve is then our AUC. A perfect model would 
    yield an AUC of 1, a random one 0.5 and a completely wrong one 0.
    For multiclass prediction models, plotting the ROC curve is not 
    really feasible. But obtaining the AUC is possible, extending the 
    definition to more classes. Considering $i=0,1,2,...,c-1$ classes 
    with $c>2$, and $p(i|x)$ the respective probability functions. 
    For any pair of classes $i$ and $j$, we define $A(i|j)$ as the 
    probability that a randomly selected observation from the jth class 
    will have a lower estimated probability of belonging to class $i$ 
    than a randomly drawn member of class $i$.
    \\\\
    For the two class case, obviously $A(i|j) = A(j|i)$. But it 
    doesn't generally hold.
    \begin{equation}
        A(i|j)=\frac{S_i-n_i(n_i+1)/2}{n_in_j},  
    \end{equation}
    where $n_i$ is the number of observations within the ith class.
    \begin{equation}
        A(i,j) = \frac{1}{2}(A(i|j)+A(j|i)).   
    \end{equation}
    Then, the overall AUC is given by the M measure, where 
    \begin{equation}
        M=\frac{2}{c(c-1)}\Sigma_{i,j}A(i,j)
    \end{equation}
    
    % \textcolor{red}{AIC}
    
    Akaike's Information Criterion, aka AIC, is obtained by the 
    following equation:
    \begin{equation}
        AIC = 2k-2ln(L)  
    \end{equation}
    where k is the number of predictors and L is the maximum value 
    of the likelihood function for the model. Goodness of fit is 
    assessed by the likelihood function and then penalized by the 
    number of parameters to avoid overfitting. Thus, between two models 
    having a similar maximum likelihood, the one with fewer parameters 
    will be preferred based on the AIC measure.